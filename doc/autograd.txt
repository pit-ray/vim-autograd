autograd.vim  Automatic differentiation library written in pure Vim script.

==============================================================================
CONTENTS                                             *vim-autograd-contents*
    1. Introduction                              |vim-autograd-introduction|
    2. Install                                        |vim-autograd-install|
    3. Classes                                        |vim-autograd-classes|
    4. Functions                                    |vim-autograd-functions|
    5. License                                        |vim-autograd-license|
    6. Maintainers                                |vim-autograd-maintainers|
==============================================================================
INTRODUCTION                                     *vim-autograd-introduction*

Automatic differentiation library written in pure Vim script.

The development site is located at https://github.com/pit-ray/vim-autograd

==============================================================================
INSTALL                                               *vim-autograd-install*
If you are using vim-plug, can install as follows. >

    Plug 'pit-ray/vim-autograd'
<
==============================================================================
CLASSES                                               *vim-autograd-classes*
    Tensor
        Tensor.backward()                                |Tensor.backward()|
        Tensor.cleargrad()                              |Tensor.cleargrad()|
        Tensor.a()                                              |Tensor.a()|
        Tensor.m()                                              |Tensor.m()|
        Tensor.s()                                              |Tensor.s()|
        Tensor.d()                                              |Tensor.d()|
        Tensor.p()                                              |Tenspr.p()|
        Tensor.n()                                              |Tensor.n()|
        Tensor.T()                                              |Tensor.T()|
        Tensor.reshape()                                  |Tensor.reshape()|
        Tensor.flatten()                                  |Tensor.flatten()|
        Tensor.clone()                                      |Tensor.clone()|
        Tensor.detach()                                    |Tensor.detach()|

    Function
        Function.apply()                                  |Function.apply()|
        Function.forward()                              |Function.forward()|
        Function.backward()                            |Function.backward()|

    NoGrad
        NoGrad.end()                                          |NoGrad.end()|

------------------------------------------------------------------------------
Tensor.backward(create_graph=0, retain_outgrad=0)         *Tensor.backward()*
Backpropagate from the current tensor to obtain the gradient.
This function accumulates the gradient in the .grad attribute of the leaves.
Therefore, multiple calling without |Tensor.cleargrad()| method will add a new
gradient into the accumulated gradient to perform double-backprop.

    Arguments:
        create_graph (`Bool`):
            Construct a computational graph for backpropergation to compute
            higher-order derivatives.

        retain_outgrad (`Bool`):
            Holds the gradient of the variable output by each function.
            if y=f(x), y.grad holds the gradient of df/dy.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#mul(x, 3.0)
        call y.backward()
<
------------------------------------------------------------------------------
Tensor.cleargrad()                                      *Tensor.cleargrad()*
Resets the retained gradient.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 3.0)
        call y.backward(1)
        let gx1 = x.grad

        call x.cleargrad()
        call gx1.backward()
        let gx2 = x.grad
<
------------------------------------------------------------------------------
Tensor.a(other)                                                 *Tensor.a()*
This method is the same as autograd#add(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.a(3.0)
<
------------------------------------------------------------------------------
Tensor.m(other)                                                 *Tensor.m()*
This method is the same as autograd#mul(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.m(3.0)
<
------------------------------------------------------------------------------
Tensor.s(other)                                                 *Tensor.s()*
This method is the same as autograd#sub(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.s(3.0)

------------------------------------------------------------------------------
Tensor.d(other)                                                 *Tensor.d()*
This method is the same as autograd#div(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
             operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.d(3.0)

------------------------------------------------------------------------------
Tensor.p(other)                                                 *Tenspr.p()*
This method is the same as autograd#pow(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.p(3.0)

------------------------------------------------------------------------------
Tensor.n()                                                      *Tensor.n()*
This method is the same as autograd#mul(self, -1.0).

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.n()

------------------------------------------------------------------------------
Tensor.T()                                                      *Tensor.T()*
This method is the same as autograd#transpose(self).

    Example: >
        let x0 = autograd#randn(2, 3)
        let x1 = autograd#randn(2, 3)
        let y = autograd#matmul(x0, x1.T())

------------------------------------------------------------------------------
Tensor.reshape(s1, s2, ..., sn), Tensor.reshape(shape)    *Tensor.reshape()*
This method is the same as autograd#reshape(self, [s1, s2, ..., sn]) or
autograd#reshape(self, shape).

    Arguments
        shape (`List` of `Number`, variadic of `Number`):
            When only one argument is given, if its type is v:t_list, it is
            considered a shape; otherwise, a:000 is considered a shape.

    Example: >
        let x0 = autograd#randn(2, 3)
        let x1 = x0.reshape(3, 2)
        let x2 = x0.reshape([3, 2])

------------------------------------------------------------------------------
Tensor.flatten()                                          *Tensor.flatten()*
This method is the same as autograd#flatten(self).

    Example: >
        let x = autograd#randn(2, 3)
        let y = x.flatten()
        " y.shape: [6]

------------------------------------------------------------------------------
Tensor.clone()                                              *Tensor.clone()*
Create a new tensor by copying the .data and .shape of the tensor. The created
tensor is completely independent of the original tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.clone()
<
------------------------------------------------------------------------------
Tensor.detach()                                            *Tensor.detach()*
It returns a new tensor detached from the current graph. However, returned
tensor shares the same data and shape attribute.

    Example: >
        let x = autograd#randn(2, 3).m(0.01).detach()
<
------------------------------------------------------------------------------
Function.apply(x1, x2, ..., xn)                           *Function.apply()*
It constructs computed graphs after calling the forward method of a
differentiable function of your own definition. If the |Function.forward()|
method returns a `List` with one element, it returns a Tensor object. If
|Function.forward()| method returns a `List` with two or more elements, it
returns a `List`.
For details, see |autograd#Function()|

    Arguments:
        x1, x2, ..., xn (variadic of Tensor, `List`, `Float`, or `Number`):
            All arguments are converted to Tensor.

------------------------------------------------------------------------------
Function.forward(xs)                                    *Function.forward()*
Forward propagation of the function.
When defining your own, you must return a `List` of Tensors in the return
value. For details, see |autograd#Function()|

    Arguments:
        xs (`List` of Tensor):
            A `List` of Tensors is passed as an argument.

------------------------------------------------------------------------------
Function.backward(gys)                                 *Function.backward()*
Backward propagation of the function.
A `List` with Tensors whose elements refer to the .grad attribute of the
Tensor output by |Function.forward()| is passed as an argument. The function
used in backward must be a differentiable Function object, and the return
value must be a `List` of Tensors.
For details, see |autograd#Function()|

    Arguments:
        gys (`List` of Tensor):
            A `List` of Tensors is passed as an argument.

------------------------------------------------------------------------------
NoGrad.end()                                                  *NoGrad.end()*
Exit from the mode without backpropagation.
NoGrad objects can be created with |autograd#no_grad()|.

    Example: >
        let x = autograd#tensor(2.0)
        let handle = autograd#no_grad()
        let y = autograd#pow(x, 4.0)
        call handle.end()
<
==============================================================================
FUNCTIONS                                           *vim-autograd-functions*
    Tensor Constructor
        autograd#tensor()                                |autograd#tensor()|
        autograd#as_tensor()                          |autograd#as_tensor()|
        autograd#zeros()                                  |autograd#zeros()|
        autograd#zeros_like()                        |autograd#zeros_like()|
        autograd#ones()                                    |autograd#ones()|
        autograd#ones_like()                          |autograd#ones_like()|
        autograd#rand()                                    |autograd#rand()|
        autograd#uniform()                              |autograd#uniform()|
        autograd#randn()                                  |autograd#randn()|
        autograd#normal()                                |autograd#normal()|

    Function Constructor
        autograd#Function()                            |autograd#Function()|

    Differentiable Functions
        autograd#add()                                      |autograd#add()|
        autograd#mul()                                      |autograd#mul()|
        autograd#sub()                                      |autograd#sub()|
        autograd#div()                                      |autograd#div()|
        autograd#pow()                                      |autograd#pow()|
        autograd#sqrt()                                    |autograd#sqrt()|
        autograd#log()                                      |autograd#log()|
        autograd#exp()                                      |autograd#exp()|
        autograd#sin()                                      |autograd#sin()|
        autograd#cos()                                      |autograd#cos()|
        autograd#tanh()                                    |autograd#tanh()|
        autograd#abs()                                      |autograd#abs()|
        autograd#sign()                                    |autograd#sign()|
        autograd#sum()                                      |autograd#sum()|
        autograd#broadcast_to()                    |autograd#broadcast_to()|
        autograd#sum_to()                                |autograd#sum_to()|
        autograd#max()                                      |autograd#max()|
        autograd#maximum()                              |autograd#maximum()|
        autograd#transpose()                          |autograd#transpose()|
        autograd#matmul()                                |autograd#matmul()|
        autograd#reshape()                              |autograd#reshape()|
        autograd#flatten()                              |autograd#flatten()|

    Mathematical Utilities
        autograd#manual_seed()                      |autograd#manual_seed()|
        autograd#random()                                |autograd#random()|
        autograd#fmax()                                    |autograd#fmax()|
        autograd#pi()                                        |autograd#pi()|
        autograd#grad()                                    |autograd#grad()|
        autograd#no_grad()                              |autograd#no_grad()|
        autograd#elementwise()                      |autograd#elementwise()|

    Utilities
        autograd#utils#shuffle()                  |autograd#utils#shuffle()|
        autograd#utils#dump_graph()            |autograd#utils#dump_graph()|
        autograd#utils#numerical_grad()    |autograd#utils#numerical_grad()|
        autograd#utils#gradcheck()              |autograd#utils#gradcheck()|

------------------------------------------------------------------------------
autograd#tensor(data)                                    *autograd#tensor()*
It creates a Tensor object based on the arguments.

    Arguments:
        data (`List`, `Float`, `Number`):
            `Float` and `Number` are generated as Tensor of shape in [1]. It
            accepts a `List` of multidimensional arrays as arguments and
            generates a Tensor of that shape. Arrays such as [[1], [2, 3]]
            will result in an error. By the way, `Number` is converted to
            `Float`.

    Example: >
        let x = autograd#tensor([[1, 2, 3], [4, 5, 6]])
        call assert_equal([2, 3], x.shape)
        call assert_equal([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], x.data)
<
------------------------------------------------------------------------------
autograd#as_tensor(data)                              *autograd#as_tensor()*
Convert data to Tensor.

    Arguments:
        data (Tensor, `List`, `Float`, `Number`):
            Tensors are returned as is, and others are converted to Tensors
            and returned.

    Example: >
        let x = autograd#as_tensor([2, 3, 4])
<
------------------------------------------------------------------------------
autograd#zeros(shape)                                     *autograd#zeros()*
Create a Tensor filled with zero with the shape given by the argument.

    Arguments:
        shape (`List`):
            `List` with `Number` as element.

    Example: >
        let x = autograd#zeros([2, 3])
<
------------------------------------------------------------------------------
autograd#zeros_like(tensor)                          *autograd#zeros_like()*
Create a Tensor filled with zero in the same shape as the argument.

    Arguments:
        tensor (Tensor):
            The shape will be the same as the output tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#zeros_like(x)
<

------------------------------------------------------------------------------
autograd#ones(shape)                                       *autograd#ones()*
Create a Tensor filled with one with the shape given by the argument.

    Arguments:
        shape (`List`):
            `List` with `Number` as element.

    Example: >
        let x = autograd#ones([2, 3])
<
------------------------------------------------------------------------------
autograd#ones_like(tensor)                            *autograd#ones_like()*
Create a Tensor filled with one in the same shape as the argument.

    Arguments:
        tensor (Tensor):
            The shape will be the same as the output tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#ones_like(x)

------------------------------------------------------------------------------
autograd#rand(s1, s2, ..., sn)                             *autograd#rand()*
Create a Tensor of the same shape as the argument with the random value
sampled from a uniform distribution as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#rand(2, 3)

------------------------------------------------------------------------------
autograd#uniform(low=0.0, high=1.0, shape=[1])          *autograd#uniform()*
Create a Tensor of the same shape as the argument with the random value
sampled from a uniform distribution between low and high range as elements.

    Arguments:
        low (`Float`):
            Lower boundary of the output interval.

        high (`Float`):
            Upper boundary of the output interval.

        shape (`List` of `Number`)
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#uniform(0.0, 5.0, [2, 3])

------------------------------------------------------------------------------
autograd#randn(s1, s2, ..., sn)                           *autograd#randn()*
Create a Tensor of the same shape as the argument with the random value
sampled from a standard uniform distribution sampled from a normal
distribution with mean 0.0 and standard deviation 1.0 as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#randn(2, 3)

------------------------------------------------------------------------------
autograd#normal(mean=0.0, std=1.0, shape=[1])            *autograd#normal()*
Generate a normal distribution with arbitrary mean and standard deviation.

    Arguments:
        mean (`Float`):
            Average value of `Float` type.

        std (`Float`):
            Standard deviation of `Float` type.

        shape (`List` of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#normal(0.0, 2.0, [2, 3])

------------------------------------------------------------------------------
autograd#Function(name)                                *autograd#Function()*
Defines a differentiable function.
At this time, you must also define functions with a dict suffix of
{name}_forward() and {name}_backward() to the name specified in the name
argument. Defining your own Function object is more efficient than using the
differentiable functions provided by vim-autograd, such as |autograd#add()| or
|autograd#mul()|, since it does not retain intermediate objects in the
computational graph. The |Function.forward()| and |Function.backward()| methods
refer to the {name}_forward and {name}_backward methods, respectively.

    Arguments:
        name (`String`):
            This argument must also have a scope prefix such as `s:`.

    Example: >
        function! nn#relu(x) abort
          return autograd#Function('nn#relu').apply(a:x)
        endfunction

        function! nn#relu_forward(xs) dict abort
          return [autograd#elementwise(a:xs, {a -> a > 0.0 ? a : 0.0})]
        endfunction

        function! nn#relu_backward(gys) dict abort
          let l:x = self.inputs[0]
          let l:mask = autograd#elementwise([l:x], {a -> a > 0.0})
          return [autograd#mul(a:gys[0], l:mask)]
        endfunction

------------------------------------------------------------------------------
autograd#add(input, other)                                  *autograd#add()*
Add like input + other element-wise. There is an alias |Tensor.a()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#add(x, 4.0)
        " y.data: [6.0]
<
------------------------------------------------------------------------------
autograd#mul(input, other)                                  *autograd#mul()*
Multiply like input * other element-wise. There is an alias |Tensor.m()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(4.0)
        let y = autograd#mul(x, 2.0)
        " y.data: [8.0]

------------------------------------------------------------------------------
autograd#sub(input, other)                                  *autograd#sub()*
Subtract like input - other element-wise. There is an alias |Tensor.s()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(5.0)
        let y = autograd#sub(x, 2.0)
        " y.data: [3.0]

------------------------------------------------------------------------------
autograd#div(input, other)                                  *autograd#div()*
Divide like input / other element-wise. There is an alias |Tensor.d()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(6.0)
        let y = autograd#div(x, 2.0)
        " y.data: [3.0]

------------------------------------------------------------------------------
autograd#pow(input, other)                                  *autograd#pow()*
Compute element-wise power like input ** other (input ^ other).
There is an alias |Tensor.p()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 5.0)
        " y.data: [32.0]
<
------------------------------------------------------------------------------
autograd#sqrt(input)                                       *autograd#sqrt()*
Calculate element-wise square root of input. This is the same as
autograd#pow(input, 0.5).

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#sqrt(x)
        " y.data: [1.414214]
<
------------------------------------------------------------------------------
autograd#log(input)                                         *autograd#log()*
Calculate the natural logarithm log that the inverse of the exponential
function in base e.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#log(x)
        " y.data: 0.693147
<
------------------------------------------------------------------------------
autograd#exp(input)                                         *autograd#exp()*
Calculate the exponential.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#exp(x)
        " y.data: 7.389056

------------------------------------------------------------------------------
autograd#sin(theta)                                         *autograd#sin()*
Calculate element-wise sine.

    Arguments:
        theta (Tensor, `List`, `Float`, `Number`):j
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(autograd#pi() / 4)
        let y = autograd#sin(x)
        " y.data: 0.707107

------------------------------------------------------------------------------
autograd#cos(theta)                                         *autograd#cos()*
Calculate element-wise cosine.

    Arguments:
        theta (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(autograd#pi() / 4)
        let y = autograd#cos(x)
        " y.data: 0.707107
<
------------------------------------------------------------------------------
autograd#tanh(input)                                       *autograd#tanh()*
Calculate element-wise hyperbolic tangent.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#tanh(x)
        " y.data: 0.964028
<
------------------------------------------------------------------------------
autograd#abs(input)                                         *autograd#abs()*
Calculate element-wise absolute value.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor([-2.0, 1.0, -3.0])
        let y = autograd#abs(x)
        " y.data: [2.0, 1.0, 3.0]
<
------------------------------------------------------------------------------
autograd#sign(input)                                       *autograd#sign()*
Compute element-wise sign function. sign(x) returns 1 when x is greater than
0, 0 when x is 0, and -1 when x is less than 0.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor([-2.0, 1.0, 0.0])
        let y = autograd#sign(x)
        " y.data: [-1.0, 1.0, 0.0]
<
------------------------------------------------------------------------------
autograd#sum(input, axis=[], keepdims=0)                    *autograd#sum()*
It returns the sum of all elements or elements in given axis in input.
If no axis is specified, it returns the sum of all elements. If keepdims is
True (1), the summed axes are kept as the axis of element 1.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

        axis (`Number`, `List` of `Number`):
            The axes must be continuous on the left or right side. For
            example, you cannot specify [1, 2] in a 4D Tensor. Also,
            specifying something like [0, 2] is interpreted the same shape as
            [0, 1, 2]. By the way, a negative axis is converted to a
            reverse-index.

        keepdims (`Bool`):
            Whether the output Tensor retains the same dimension as
            input.shape.

    Example: >
        let x = autograd#tensor([[1, 2, 3], [4, 5, 6]])
        let y = autograd#sum(x, 1)
        " y.data: [6.0, 15.0]
<
------------------------------------------------------------------------------
autograd#broadcast_to(input, shape)                *autograd#broadcast_to()*
Broadcasts input Tensor to the shape. However, the axis to be broadcast must
be biased to the left or right, so broadcasts on the middle axis are not
supported. For example, [1, 1, 2] can be broadcast to [4, 2, 2], but [4, 1, 2]
cannot be broadcast to [4, 2, 2].

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

        shape (`List` of `Number`):
            The new shape.

    Example: >
        let x = autograd#randn(3, 5, 1)
        let y = autograd#broadcast_to(x, [3, 5, 7])
<
------------------------------------------------------------------------------
autograd#sum_to(input, shape)                            *autograd#sum_to()*
For backpropagation of broadcast_to, sum input so that it becomes shape. This
function is used to backpropagate of implicit broadcasts in
|autograd#elementwise()| when you define your own Function object.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        function! nn#add(x0, x1) abort
          return autograd#Function('s:add').apply(a:x0, a:x1)
        endfunction

        function! nn#add_forward(xs) dict abort
          return [autograd#elementwise(a:xs, {a, b -> a + b})]
        endfunction

        function! nn#add_backward(gys) dict abort
          let l:x0 = self.inputs[0]
          let l:x1 = self.inputs[1]

          let l:gx0 = a:gys[0]
          let l:gx1 = a:gys[0]

          if l:x0.shape == l:x1.shape
            return [l:gx0, l:gx1]
          endif
          return [
            \ autograd#sum_to(l:gx0, l:x0.shape),
            \ autograd#sum_to(l:gx1, l:x1.shape)
            \ ]
        endfunction
<
------------------------------------------------------------------------------
autograd#max(input)                                         *autograd#max()*
Returns the maximum value of all elements of input.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor([[2.0, 5.6], [2.5, -2.0]])
        let y = autograd#max(x)
        " y.data: [5.6]

------------------------------------------------------------------------------
autograd#maximum(input, other)                          *autograd#maximum()*
Returns the element-wise maximum of input and other.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let a = autograd#tensor([1, 2, -1])
        let b = autograd#tensor([3, 0, 4])
        let y = autograd#maximum(a, b)
        " y.data: [3.0, 2.0, 4.0]

------------------------------------------------------------------------------
autograd#transpose(input)                             *autograd#transpose()*
Returns a new Tensor transposed. There is an alias |Tensor.T()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#transpose(x)
        " y.shape: [3, 2]

------------------------------------------------------------------------------
autograd#matmul(input, other)                            *autograd#matmul()*
Matrix product of two tensors.
* If both Tensors are 1-dimensional, the dot product is returned.
* If both Tensors are 2-dimensional, the matrix product is returned.
* If the Tensors are 2-dimensional and 1-dimensional, a new axis is added to
  the 1-dimensional Tensor and the matrix product is returned. If the left
  operand is 1-dimensional, it is converted to 1 x n, and if the right operand
  is 1-dimensional, it is converted to n x 1.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. For a matrix
            product, input.shape[1] must match other.shape[0].

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. For a matrix
            product, other.shape[0] must match input.shape[1].

    Example: >
        let x = autograd#randn(2, 3)
        let W = autograd#randn(3, 2)
        let y = autograd#matmul(x, W)
<
------------------------------------------------------------------------------
autograd#reshape(input, shape)                          *autograd#reshape()*
Returns Tensor of different shapes that share the same data as input.
There is an alias |Tensor.reshape()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

        shape (`List` of `Number`):
            The new shape. However, the flatten size of the shape and the
            number of elements of input.data must match.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#reshape(x, [3, 2])
<
------------------------------------------------------------------------------
autograd#flatten(input)                                 *autograd#flatten()*
Reshape Tensor to one-dimensional.
This is the same as autograd#reshape(input, [len(input.data)]).
There is an alias |Tensor.flatten()|.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#flatten(x)
        " y.shape: [6]
<
------------------------------------------------------------------------------
autograd#manual_seed(seed)                          *autograd#manual_seed()*
It sets the seed of the random functions provided by vim-autograd. By default,
the seed is a random value returned by `srand()`.

    Arguments:
        seed (`Number`):
            It uses `srand(seed)`.

    Example: >
        call autograd#manual_seed(42)

------------------------------------------------------------------------------
autograd#random()                                        *autograd#random()*
It returns a random scalar value between 0.0 and 1.0.

    Example: >
        let x = autograd#random()
<
------------------------------------------------------------------------------
autograd#fmax(list_obj)                                    *autograd#fmax()*
It returns the maximum value of a `List` whose elements are `Float` values.

    Arguments:
         list_obj (`List`):
             A list of `Float' or `Number`.

    Example: >
        let x = autograd#fmax([2, 3, 4, 5, 6])
<
------------------------------------------------------------------------------
autograd#pi()                                                *autograd#pi()*
It returns a scalar value that approximates pi.

    Example: >
        let x = autograd#pi()
<
------------------------------------------------------------------------------
                                                           *autograd#grad()*
autograd#grad(output, inputs, create_graph=0, retain_outgrad=0)
Compute and return the gradient of the output with respect to the inputs. 

    Arguments:
        output (Tensor):
            A Tensor object that retained the computed graph.

        inputs (Tensor, `List` of Tensor):
            If it is a `List` with one element or Tensor, a single Tensor is
            returned. And, if a `List` with multiple elements, a `List` of
            Tensor is returned. The .grad attribute of this inputs argument is
            not polluted.

        create_graph (`Bool`):
            Construct a computational graph for backpropergation to compute
            higher-order derivatives.

        retain_outgrad (`Bool`):
            Holds the gradient of the variable output by each function.
            if y=f(x), y.grad holds the gradient of df/dy.


    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 5.0)
        let gx1 = autograd#grad(y, x, 1)
        let gx2 = autograd#grad(gx1, x, 1)
        " y.data  : [32.0]
        " gx1.data: [80.0]
        " gx2.data: [160.0]
<
------------------------------------------------------------------------------
autograd#no_grad()                                      *autograd#no_grad()*
Switch to a mode that does not create computed graphs to save computation cost
and memory consumption. Backpropagation is not available for functions
computed in this mode. To exit this mode, call the |NoGrad.end()| method of
the returned handle.

    Example: >
        let x = autograd#tensor(2.0)
        let handle = autograd#no_grad()
        let y = autograd#pow(x, 4.0)
        call handle.end()

------------------------------------------------------------------------------
autograd#elementwise(inputs, f, out={})             *autograd#elementwise()*
Create a tensor whose elements are the result of feeding each element of
inputs into f. However, this function corresponds to the constructor of Tensor
and is not differentiable.

    Arguments:
        inputs (`List` of Tensor):
            If the number of elements is one, process f as a unary operator,
            and if is two, do as a binary operator. If the Tensor shapes do
            not match, the smaller shape is automatically broadcast to the
            larger shape.

        f (`lambda`, `Funcref`):
            A function reference or lambda that takes one or two scalar values
            and returns one scalar value.

        out (`Tensor`, optional):
            Tensor object with the same shape with the larger shape in
            inputs. If this argument is empty, a new Tensor object is created
            and the results are stored. If a valid Tensor object is passed as
            of this argument, stores the element-wise results to its .data
            attribute, so the Tensor's id attribute does not change and can
            save computational cost. In both cases, the result Tensor is
            returned as the return value.

    Example: >
        let a = autograd#tensor([[2, 3], [4, 5]])
        let b = autograd#tensor([[7, 6], [2, 5]])
        let y = autograd#elementwise([a, b], {p, q -> p + q})
        " y.shape: [2, 2]
        " y.data: [9, 9, 6, 10]
<
------------------------------------------------------------------------------
autograd#utils#shuffle(list_obj)                  *autograd#utils#shuffle()*
Sort the list randomly. It based on Fisher-Yates shuffle algorithm,

    Arguments:
        list_obj (`List`):
            A list of `Float` or `Number`.

    Example: >
        let x = autograd#utils#shuffle(range(10))
        " x: [8, 5, 1, 6, 4, 7, 9, 3, 2, 0]
<
------------------------------------------------------------------------------
autograd#utils#dump_graph(last_node, filepath) *autograd#utils#dump_graph()*
Visualize computational graphs of last_node.
If graphviz is installed, it generates images and DOT language source code,
otherwise it generates only DOT language source code. If you want to give a
label to a node, set a string to the .name attribute of the Tensor object
contributing to the graph.

    Arguments:
        last_node (Tensor):
            Last node of the computational graph.

        filepath (`String`): 
            The path to the the image to be generated. If there is no parent
            directory of the file, it will be generated recursively.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 3.0)
        let x.name = 'x'
        let y.name = 'y'
        call autograd#utils#dump_graph(y, '.autograd/pow.png')

------------------------------------------------------------------------------
autograd#utils#numerical_grad(f, input)    *autograd#utils#numerical_grad()*
Compute and return the numerical derivative based on central difference.

    Arguments:
        f (`lambda`, `Funcref`):
            A function object that accepts a single Tensor. If the
            multivariable function, fix all but one argument using a lambda.

        x (Tensor):
            Tensor fed to f.

    Example: >
        let a = autograd#tensor(2.0)
        let b = autograd#tensor(6.0)
        let F = {x -> autograd#mul(x, b)}
        let grad = autograd#utils#numerical_grad(F, a)

------------------------------------------------------------------------------
autograd#utils#gradcheck(f, inputs)             *autograd#utils#gradcheck()*
It compares the result of the numerical differentiation with the gradient and
exits successfully if all elements are very close, and emits an assert if they
are different.

    Arguments:
        f (`lambda`, `Funcref`):
            A differentiable function that accepts a `List` of Tensors.

        inputs (`List` of Tensor):
            `List` of Tensor fed to f.

    Example: >
        let a = autograd#randn(2, 3)
        let b = autograd#randn(2, 3)
        let F = {xs -> autograd#mul(xs[0], xs[1])}
        call autograd#utils#gradcheck(F, [a, b])

==============================================================================
LICENSE                                               *vim-autograd-license*

MIT License Copyright (c) 2022 pit-ray

The full text is available at
https://github.com/pit-ray/vim-autograd/blob/main/LICENSE.

==============================================================================
MAINTAINERS                                       *vim-autograd-maintainers*

* pit-ray (author, maintainer): https://github.com/pit-ray


vim:tw=78:ts=8:ft=help:norl:
