autograd.vim  Automatic differentiation library written in pure Vim9 script.

==============================================================================
CONTENTS                                             *vim-autograd-contents*
    1. Introduction                              |vim-autograd-introduction|
    2. Install                                        |vim-autograd-install|
    3. Classes                                        |vim-autograd-classes|
    4. Functions                                    |vim-autograd-functions|
    5. License                                        |vim-autograd-license|
    6. Maintainers                                |vim-autograd-maintainers|
==============================================================================
INTRODUCTION                                     *vim-autograd-introduction*

Automatic differentiation library written in pure Vim9 script.

The development site is located at https://github.com/pit-ray/vim-autograd

==============================================================================
INSTALL                                               *vim-autograd-install*
If you are using vim-plug, can install as follows. >

    Plug 'pit-ray/vim-autograd', {'branch': 'vim9'}
<
==============================================================================
CLASSES                                               *vim-autograd-classes*
    Tensor
        Tensor.new                                              |Tensor.new|
        Tensor.data                                            |Tensor.data|
        Tensor.grad                                            |Tensor.grad|
        Tensor.shape                                          |Tensor.shape|
        Tensor.Numel()                                      |Tensor.Numel()|
        Tensor.Dim()                                          |Tensor.Dim()|
        Tensor.Empty()                                      |Tensor.Empty()|
        Tensor.ClearGrad()                              |Tensor.ClearGrad()|
        Tensor.SetName()                                  |Tensor.SetName()|

------------------------------------------------------------------------------
Tensor.new(data)                                              *Tensor.new()*
Creates a Tensor object based on the arguments.

    Arguments:
        data (`list`<`float`>, `list`<`number`>, `float`, `number`):
            `float` and `number` are generated as Tensor of shape in [1]. It
            accepts a `list` of multidimensional arrays as arguments and
            generates a Tensor of that shape. Arrays such as [[1], [2, 3]]
            will result in an error. By the way, `number` is converted to
            `float`.

    Example: >
        import 'autograd.vim' as ag

        var x = ag.Tensor.new([[1, 2, 3], [4, 5, 6]])
        echo x.shape  # [2, 3]
        echo x.data   # [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]

------------------------------------------------------------------------------
Tensor.data                                                     *Tensor.data*
This attribute is the raw data retained as a 1-dimensional `list` of `float`.
If you create a multidimensional array using |autograd.Tensor.new()| or
|autograd.Zeros()|, etc., everything will be converted to flatten vector.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new([[1, 2], [3, 4]])
        echo x.data  # [1.0, 2.0, 3.0, 4.0]

------------------------------------------------------------------------------
Tensor.grad                                                     *Tensor.grad*
This attribute holds the gradient of that variable for a given output. Since
it is a nested Tensor object, you can access the raw data as a |Tensor.data|
attribute. However, the this attribute of each Tensor object on a computed
graph that has not called |autograd.Backward()| or |autograd.Grad()| is null.
In v9.0.1558, |Tensor.grad| is declared as `any` type because a class cannot
hold its own class nested as a member when it is declaring. Therefore, please
indicate the type of |Tensor.grad| explicity as follows.

    Example: >
        import 'autograd.vim' as ag
        const Tensor = ag.Tensor

        var x = ag.Tensor.new(2.0)
        echo x.grad   # null

        var y = ag.Mul(x, 5.0)
        ag.Backward(y)

        var gx: Tensor = x.grad
        echo gx.data  # [5.0]
<
------------------------------------------------------------------------------
Tensor.shape                                                   *Tensor.shape*
This attribute has a `list` of `number` representing the shape of the array.
Since vim-autograd does not have the concept of a scalar, a tensor created by
|autograd.Tensor.new()| given a scalar value is interpreted as a vector of size 1.

    Example: >
        import 'autograd.vim' as ag

        var x = ag.Tensor.new(2.0)
        echo x.shape  # [1]
        echo x.data   # [2.0]

        var y = ag.Tensor.new([[1, 2], [3, 4]])
        echo y.shape  # [2, 2]
        echo y.data   # [1.0, 2.0, 3.0, 4.0]

------------------------------------------------------------------------------
Tensor.Numel()                                               *Tensor.Numel()*
Returns the total number of elements in the |Tensor.data| attribute. This
method is equivalent to len(x.data).

    Example: >
        import 'autograd.vim' as ag

        var x1 = ag.Tensor.new(2.0)
        echo x1.shape    # [1]
        echo x1.Numel()  # 1

        var x2 = ag.Tensor.new([1, 2, 3, 4])
        echo x2.shape    # [4]
        echo x2.Numel()  # 4

        var x3 = ag.Tensor.new([[1, 2], [3, 4], [5, 6]])
        echo x3.shape    # [3, 2]
        echo x3.Numel()  # 6

------------------------------------------------------------------------------
Tensor.Dim()                                                   *Tensor.Dim()*
Returns the number of dimensions of the |Tensor.data| attribute. This method
is equivalent to len(x.shape).

    Example: >
        import 'autograd.vim' as ag

        var x1 = ag.Tensor.new([1, 2, 3, 4])
        echo x1.shape  # [4]
        echo x1.Dim()  # 1

        var x2 = ag.Tensor.new([[1, 2], [3, 4], [5, 6]])
        echo x2.shape  # [3, 2]
        echo x2.Dim()  # 2

------------------------------------------------------------------------------
Tensor.Empty()                                               *Tensor.Empty()*
Checks if the |Tensor.data| attribute is empty and returns it with a `bool`
value. This method is equivalent to x.Numel() == 0.

    Example: >
        import 'autograd.vim' as ag

        var x1 = ag.Tensor.new([1, 2, 3, 4])
        echo x1.Empty()  # false

        var x2 = ag.Tensor.new([])
        echo x2.Empty()  # true

------------------------------------------------------------------------------
Tensor.ClearGrad()                                       *Tensor.ClearGrad()*
Resets the retained gradient.

    Example: >
        import 'autograd.vim' as ag
        const Tensor = ag.Tensor

        var x = ag.Tensor.new(2.0)
        var y = ag.Pow(x, 3.0)
        ag.Backward(y, true)
        var gx1: Tensor = x.grad
        echo gx1.data  # [12.0]

        x.ClearGrad()
        echo x.grad  # null

        ag.Backward(gx1)
        var gx2: Tensor = x.grad
        echo gx2.data  # [12.0]
<
------------------------------------------------------------------------------
Tensor.SetName()                                           *Tensor.SetName()*
Set the name to the Tensor node for visualization of the computational graph
by |autograd.DumpGraph()|.

    Example: >
        import 'autograd.vim' as ag
        const Tensor = ag.Tensor

        var x = Tensor.new(2.0)
        var y = ag.Add(x, 3.0)
        ag.Backward(y)

        x.SetName('x')
        y.SetName('y')
        ag.DumpGraph(y, '.autograd/graph.png')


==============================================================================
FUNCTIONS                                           *vim-autograd-functions*
    Tensor Constructor
        autograd.AsTensor()                            |autograd.AsTensor()|
        autograd.Zeros()                                  |autograd.Zeros()|
        autograd.ZerosLike()                          |autograd.ZerosLike()|
        autograd.Ones()                                    |autograd.Ones()|
        autograd.OnesLike()                            |autograd.OnesLike()|
        autograd.Rand()                                    |autograd.Rand()|
        autograd.Uniform()                              |autograd.Uniform()|
        autograd.Randn()                                  |autograd.Randn()|
        autograd.Normal()                                |autograd.Normal()|

    Differentiable Functions
        autograd.Add()                                      |autograd.Add()|
        autograd.Mul()                                      |autograd.Mul()|
        autograd.Sub()                                      |autograd.Sub()|
        autograd.Div()                                      |autograd.Div()|
        autograd.Pow()                                      |autograd.Pow()|
        autograd.Sqrt()                                    |autograd.Sqrt()|
        autograd.Log()                                      |autograd.Log()|
        autograd.Exp()                                      |autograd.Exp()|
        autograd.Sin()                                      |autograd.Sin()|
        autograd.Cos()                                      |autograd.Cos()|
        autograd.Tanh()                                    |autograd.Tanh()|
        autograd.Abs()                                      |autograd.Abs()|
        autograd.Sign()                                    |autograd.Sign()|
        autograd.Sum()                                      |autograd.Sum()|
        autograd.BroadcastTo()                      |autograd.BroadcastTo()|
        autograd.Max()                                      |autograd.Max()|
        autograd.Maximum()                              |autograd.Maximum()|
        autograd.Transpose()                          |autograd.Transpose()|
        autograd.Matmul()                                |autograd.Matmul()|
        autograd.Reshape()                              |autograd.Reshape()|
        autograd.Flatten()                              |autograd.Flatten()|

    Mathematical Utilities
        autograd.ManualSeed()                        |autograd.ManualSeed()|
        autograd.Random()                                |autograd.Random()|
        autograd.Pi()                                        |autograd.Pi()|
        autograd.Grad()                                    |autograd.Grad()|
        autograd.NoGrad()                                |autograd.NoGrad()|
        autograd.Elementwise()                      |autograd.Elementwise()|

    Utilities
        autograd.Shuffle()                              |autograd.Shuffle()|
        autograd.DumpGraph()                          |autograd.DumpGraph()|
        autograd.NumericalGrad()                  |autograd.NumericalGrad()|
        autograd.GradCheck()                          |autograd.GradCheck()|

------------------------------------------------------------------------------
autograd.AsTensor(data)                                *autograd.AsTensor()*
Convert data to Tensor.

    Arguments:
        data (|Tensor|, `list`, `float`, `number`):
            Tensors are returned as is, and others are converted to Tensors
            and returned.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.AsTensor([2, 3, 4])
<
------------------------------------------------------------------------------
autograd.Zeros(shape)                                      *autograd.Zeros()*
Create a Tensor filled with zero with the shape given by the argument.

    Arguments:
        shape (`list`):
            `list` with `number` as element.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Zeros([2, 3])
<
------------------------------------------------------------------------------
autograd.ZerosLike(tensor)                             *autograd.ZerosLike()*
Create a Tensor filled with zero in the same shape as the argument.

    Arguments:
        tensor (|Tensor|):
            The shape will be the same as the output tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var y = ag.ZerosLike(x)

------------------------------------------------------------------------------
autograd.Ones(shape)                                        *autograd.Ones()*
Create a Tensor filled with one with the shape given by the argument.

    Arguments:
        shape (`list`):
            `list` with `number` as element.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Ones([2, 3])
<
------------------------------------------------------------------------------
autograd.OnesLike(tensor)                               *autograd.OnesLike()*
Create a Tensor filled with one in the same shape as the argument.

    Arguments:
        tensor (|Tensor|):
            The shape will be the same as the output tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var y = ag.OnesLike(x)

------------------------------------------------------------------------------
autograd.Rand(s1, s2, ..., sn)                              *autograd.Rand()*
Create a Tensor of the same shape as the argument with the random value
sampled from a uniform distribution as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `number`):
            The output tensor has the same as this shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Rand(2, 3)

------------------------------------------------------------------------------
autograd.Uniform(low=0.0, high=1.0, shape=[1])           *autograd.Uniform()*
Create a |Tensor| of the same shape as the argument with the random value
sampled from a uniform distribution between low and high range as elements.

    Arguments:
        low (`float`):
            Lower boundary of the output interval.

        high (`float`):
            Upper boundary of the output interval.

        shape (`list` of `number`)
            The output tensor has the same as this shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Uniform(0.0, 5.0, [2, 3])

------------------------------------------------------------------------------
autograd.Randn(s1, s2, ..., sn)                            *autograd.Randn()*
Create a |Tensor| of the same shape as the argument with the random value
sampled from a standard uniform distribution sampled from a normal
distribution with mean 0.0 and standard deviation 1.0 as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `number`):
            The output tensor has the same as this shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)

------------------------------------------------------------------------------
autograd.Normal(mean=0.0, std=1.0, shape=[1])             *autograd.Normal()*
Generate a normal distribution with arbitrary mean and standard deviation.

    Arguments:
        mean (`float`):
            Average value of `float` type.

        std (`float`):
            Standard deviation of `float` type.

        shape (`list` of `number`):
            The output tensor has the same as this shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Normal(0.0, 2.0, [2, 3])

------------------------------------------------------------------------------
autograd.Add(input, other)                                   *autograd.Add()*
Add like input + other element-wise.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Add(x, 4.0)
        echo y.data  # [6.0]
<
------------------------------------------------------------------------------
autograd.Mul(input, other)                                   *autograd.Mul()*
Multiply like input * other element-wise.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(4.0)
        var y = ag.Mul(x, 2.0)
        echo y.data  # [8.0]

------------------------------------------------------------------------------
autograd.Sub(input, other)                                   *autograd.Sub()*
Subtract like input - other element-wise.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(5.0)
        var y = ag.Sub(x, 2.0)
        echo y.data  # [3.0]

------------------------------------------------------------------------------
autograd.Div(input, other)                                   *autograd.Div()*
Divide like input / other element-wise.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(6.0)
        var y = ag.Div(x, 2.0)
        echo y.data  # [3.0]

------------------------------------------------------------------------------
autograd.Pow(input, other)                                   *autograd.Pow()*
Compute element-wise power like input ** other (input ^ other).

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Pow(x, 5.0)
        ehco y.data  # [32.0]
<
------------------------------------------------------------------------------
autograd.Sqrt(input)                                        *autograd.Sqrt()*
Calculate element-wise square root of input. This is the same as
|autograd.Pow|(input, 0.5).

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Sqrt(x)
        echo y.data  # [1.414214]
<
------------------------------------------------------------------------------
autograd.Log(input)                                          *autograd.Log()*
Calculate the natural logarithm log that the inverse of the exponential
function in base Napier's constant.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Log(x)
        echo y.data  # [0.693147]
<
------------------------------------------------------------------------------
autograd.Exp(input)                                          *autograd.Exp()*
Calculate the exponential.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Exp(x)
        echo y.data  # [7.389056]

------------------------------------------------------------------------------
autograd.Sin(theta)                                          *autograd.Sin()*
Calculate element-wise sine.

    Arguments:
        theta (|Tensor|, `list`, `float`, `number`):j
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(autograd.Pi() / 4)
        var y = ag.Sin(x)
        echo y.data  # [0.707107]

------------------------------------------------------------------------------
autograd.Cos(theta)                                          *autograd.Cos()*
Calculate element-wise cosine.

    Arguments:
        theta (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(autograd.Pi() / 4)
        var y = ag.Cos(x)
        echo y.data  # [0.707107]
<
------------------------------------------------------------------------------
autograd.Tanh(input)                                        *autograd.Tanh()*
Calculate element-wise hyperbolic tangent.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Tanh(x)
        echo y.data  # [0.964028]
<
------------------------------------------------------------------------------
autograd.Abs(input)                                          *autograd.Abs()*
Calculate element-wise absolute value.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a Tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new([-2.0, 1.0, -3.0])
        var y = ag.Abs(x)
        echo y.data  # [2.0, 1.0, 3.0]
<
------------------------------------------------------------------------------
autograd.Sign(input)                                        *autograd.Sign()*
Compute element-wise sign function. sign(x) returns 1 when x is greater than
0, 0 when x is 0, and -1 when x is less than 0.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

    example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new([-2.0, 1.0, 0.0])
        var y = ag.Sign(x)
        echo y.data  # [-1.0, 1.0, 0.0]
<
------------------------------------------------------------------------------
autograd.Sum(input, axis=[], keepdims=0)                     *autograd.Sum()*
It returns the sum of all elements or elements in given axis in input.
If no axis is specified, it returns the sum of all elements. If keepdims is
true (1), the summed axes are kept as the axis of element 1.

    arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

        axis (`number`, `list`<`number`>):
            The axes must be continuous on the left or right side. For
            example, you cannot specify [1, 2] in a 4D tensor. Also,
            specifying something like [0, 2] is interpreted the same shape as
            [0, 1, 2]. By the way, a negative axis is converted to a
            reverse-index.

        keepdims (`bool`):
            Whether the output tensor retains the same dimension as
            input.shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new([[1, 2, 3], [4, 5, 6]])
        var y = ag.Sum(x, 1)
        echo y.data  # [6.0, 15.0]
<
------------------------------------------------------------------------------
autograd.BroadcastTo(input, shape)                   *autograd.BroadcastTo()*
Broadcasts input tensor to the shape. However, the axis to be broadcast must
be biased to the left or right, so broadcasts on the middle axis are not
supported. For example, [1, 1, 2] can be broadcast to [4, 2, 2], but [4, 1, 2]
cannot be broadcast to [4, 2, 2].

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

        shape (`list`<`number`>):
            The new shape.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(3, 5, 1)
        var y = ag.BroadcastTo(x, [3, 5, 7])
<
------------------------------------------------------------------------------
autograd.Max(input)                                          *autograd.Max()*
Returns the maximum value of all elements of input.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new([[2.0, 5.6], [2.5, -2.0]])
        var y = ag.Max(x)
        echo y.data  # [5.6]

------------------------------------------------------------------------------
autograd.Maximum(input, other)                           *autograd.Maximum()*
Returns the element-wise maximum of input and other.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        import 'autograd.vim' as ag
        var a = ag.Tensor.new([1, 2, -1])
        var b = ag.Tensor.new([3, 0, 4])
        var y = ag.Maximum(a, b)
        echo y.data  # [3.0, 2.0, 4.0]

------------------------------------------------------------------------------
autograd.Transpose(input)                              *autograd.Transpose()*
Returns a new tensor transposed.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var y = ag.Transpose(x)
        echo y.shape  # [3, 2]

------------------------------------------------------------------------------
autograd.Matmul(input, other)                             *autograd.Matmul()*
Matrix product of two tensors.
* If both tensors are 1-dimensional, the dot product is returned.
* If both tensors are 2-dimensional, the matrix product is returned.
* If the tensors are 2-dimensional and 1-dimensional, a new axis is added to
  the 1-dimensional tensor and the matrix product is returned. If the left
  operand is 1-dimensional, it is converted to 1 x n, and if the right operand
  is 1-dimensional, it is converted to n x 1.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor. For a matrix
            product, input.shape[1] must match other.shape[0].

        other (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor. For a matrix
            product, other.shape[0] must match input.shape[1].

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var w = ag.Randn(3, 2)
        var y = ag.Matmul(x, w)
<
------------------------------------------------------------------------------
autograd.Reshape(input, shape)                           *autograd.Reshape()*
Returns tensor of different shapes that share the same data as input.

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

        shape (`list`<`number`>):
            The new shape. however, the flatten size of the shape and the
            number of elements of input.data must match.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var y = ag.Reshape(x, [3, 2])
<
------------------------------------------------------------------------------
autograd.Flatten(input)                                  *autograd.Flatten()*
Reshape tensor to one-dimensional.
This is the same as |autograd.Reshape|(input, [input.Numel()]).

    Arguments:
        input (|Tensor|, `list`, `float`, `number`):
            This argument is automatically converted to a tensor.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Randn(2, 3)
        var y = ag.Flatten(x)
        echo y.shape  # [6]
<
------------------------------------------------------------------------------
autograd.ManualSeed(seed)                            *autograd.manual_seed()*
Sets the seed of the random functions provided by vim-autograd. By default,
the seed is a random value returned by `srand()`.

    Arguments:
        seed (`number`):
            It uses `srand(seed)`.

    Example: >
        import 'autograd.vim' as ag
        ag.ManualSeed(42)

------------------------------------------------------------------------------
autograd.Random()                                         *autograd.Random()*
Returns a random scalar value between 0.0 and 1.0.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Random()
<
------------------------------------------------------------------------------
autograd.Pi()                                                 *autograd.Pi()*
Returns a scalar value that approximates pi.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Pi()
<
------------------------------------------------------------------------------
                                                            *autograd.Grad()*
autograd.Grad(output, inputs, create_graph=false, retain_outgrad=false)
Compute and return the gradient of the output with respect to the inputs. 

    Arguments:
        output (|Tensor|):
            A tensor object that retained the computed graph.

        inputs (|Tensor|, `list`<|Tensor|>):
            If it is a `list` with one element or tensor, a single tensor is
            returned. and, If a `list` with multiple elements, a `list` of
            tensor is returned. The .grad attribute of this inputs argument
            is not polluted.

        create_graph (`bool`):
            Construct a computational graph for backpropergation to compute
            higher-order derivatives.

        retain_outgrad (`bool`):
            Holds the gradient of the variable output by each function.
            If y=f(x), y.grad holds the gradient of df/dy.


    Example: >
        import 'autograd.vim' as ag
        const Tensor = ag.Tensor

        var x = ag.Tensor.new(2.0)
        var y = ag.Pow(x, 5.0)
        var gx1: Tensor = ag.Grad(y, x, 1)
        var gx2: Tensor = ag.Grad(gx1, x, 1)
        echo y.data    # [32.0]
        echo gx1.data  # [80.0]
        echo gx2.data  # [160.0]
<
------------------------------------------------------------------------------
autograd.NoGrad(Fn)                                       *autograd.NoGrad()*
Switch to a mode that does not create computed graphs to save computation cost
and memory consumption and call the Fn. The back propagation is not available
for functions computed in this mode.

    Arguments:
        Fn (`func`):
            A function to call without back propagation.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        ag.NoGrad(() => {
          var y = ag.Pow(x, 4.0)
        })

------------------------------------------------------------------------------
autograd.Elementwise(inputs, Fn, out={})             *autograd.Elementwise()*
Create a tensor whose elements are the result of feeding each element of
inputs into ElementalFunc. However, this function corresponds to the
constructor of tensor and is not differentiable.

    Arguments:
        inputs (`list`<|Tensor|>):
            If the number of elements is one, process Fn as a unary operator,
            and if is two, do as a binary operator. If the tensor shapes do
            not match, the smaller shape is automatically broadcast to the
            larger shape.

        Fn (`func`: `float`):
            A function reference or lambda that takes one or two scalar values
            and returns one `float` value.

        out (|Tensor|, optional):
            Tensor object with the same shape with the larger shape in inputs.
            If this argument is empty, a new tensor object is created and the
            results are stored. If a valid tensor object is passed as of this
            argument, stores the element-wise results to its |autograd.data|
            attribute, so the |Tensor|'s id attribute does not change and can
            save computational cost. In both cases, the result tensor is
            returned as the return value.

    Example: >
        import 'autograd.vim' as ag
        var a = ag.Tensor.new([[2, 3], [4, 5]])
        var b = ag.Tensor.new([[7, 6], [2, 5]])
        var y = ag.Elementwise([a, b], (p, q): float => p + q)
        echo y.shape  # [2, 2]
        echo y.data   # [9.0, 9.0, 6.0, 10.0]
<
------------------------------------------------------------------------------
autograd.Shuffle(list_obj)                               *autograd.Shuffle()*
Sort the list randomly. It based on Fisher-Yates shuffle algorithm.

    Arguments:
        list_obj (`list`<`flaot`>, `list`<`number`>):
            A list of `float` or `number` to sort.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Shuffle(range(10))
        echo x  # [8, 5, 1, 6, 4, 7, 9, 3, 2, 0]
<
------------------------------------------------------------------------------
autograd.DumpGraph(last_node, filepath)               *autograd.dump_graph()*
Visualize computational graphs of last_node.
If graphviz is installed, it generates images and dot language source code,
otherwise it generates only dot language source code. If you want to give a
label to a node, set a string of the name with |Tensor.SetName()|.

    Arguments:
        last_node (|Tensor|):
            Last node of the computational graph.

        filepath (`string`): 
            The path to the the image to be generated. If there is no parent
            directory of the file, it will be generated recursively.

    Example: >
        import 'autograd.vim' as ag
        var x = ag.Tensor.new(2.0)
        var y = ag.Pow(x, 3.0)
        x.SetName('x')
        y.SetName('y')
        ag.DumpGraph(y, '.autograd/pow.png')

------------------------------------------------------------------------------
autograd.NumericalGrad(Fn, input)                  *autograd.NumericalGrad()*
Compute and return the numerical derivative based on central difference.

    Arguments:
        Fn (`func`):
            A function object that accepts a single tensor. If the
            multivariable function, fix all but one argument using a lambda.

        input (tensor):
            Tensor fed to Fn.

    Example: >
        import 'autograd.vim' as ag
        var a = ag.Tensor.new(2.0)
        var b = ag.Tensor.new(6.0)
        var Fn = (x) => ag.Mul(x, b)
        var grad = ag.NumericalGrad(Fn, a)

------------------------------------------------------------------------------
autograd.GradCheck(Fn, inputs)                         *autograd.GradCheck()*
It compares the result of the numerical differentiation with the gradient and
exits successfully if all elements are very close, and emits an assert if they
are different.

    Arguments:
        Fn (`func`):
            a differentiable function that accepts a `list` of tensors.

        inputs (`list`<|Tensor|>):
            `list` of Tensor fed to f.

    Example: >
        import 'autograd.vim' as ag
        var a = ag.Randn(2, 3)
        var b = ag.Randn(2, 3)
        var Fn = (xs) => ag.Mul(xs[0], xs[1])
        ag.GradCheck(Fn, [a, b])

==============================================================================
LICENSE                                                *vim-autograd-license*

MIT License Copyright (c) 2022-2023 pit-ray

The full text is available at
https://github.com/pit-ray/vim-autograd/blob/main/license.

==============================================================================
MAINTAINERS                                        *vim-autograd-maintainers*

* pit-ray (author, maintainer): https://github.com/pit-ray


vim:tw=78:ts=8:ft=help:norl:
