autograd.vim  Automatic differentiation library written in pure vimscript.

==============================================================================
CONTENTS                                             *vim-autograd-contents*
    1. Introduction                              |vim-autograd-introduction|
    2. Install                                        |vim-autograd-install|
    3. CLASSES                                        |vim-autograd-classes|
    4. Functions                                    |vim-autograd-functions|
    5. License                                        |vim-autograd-license|
    6. Maintainers                                |vim-autograd-maintainers|
==============================================================================
INTRODUCTION                                     *vim-autograd-introduction*

==============================================================================
INSTALL                                               *vim-autograd-install*
If you are using plug.vim, can install as follows. >
    Plug 'pit-ray/vim-autograd'
<

==============================================================================
CLASSES                                               *vim-autograd-classes*
    Tensor
        Tensor.backward()                                |Tensor.backward()|
        Tensor.cleargrad()                              |Tensor.cleargrad()|
        Tensor.a()                                              |Tensor.a()|
        Tensor.m()                                              |Tensor.m()|
        Tensor.s()                                              |Tensor.s()|
        Tensor.d()                                              |Tensor.d()|
        Tensor.p()                                              |Tenspr.p()|
        Tensor.n()                                              |Tensor.n()|
        Tensor.T()                                              |Tensor.T()|
        Tensor.reshape()                                  |Tensor.reshape()|
        Tensor.clone()                                      |Tensor.clone()|
        Tensor.detach()                                    |Tensor.detach()|

    Function
        Function.apply()                                  |Function.apply()|
        Function.forward()                              |Function.forward()|
        Function.backward()                            |Function.backward()|

    NoGrad
        NoGrad.end()                                          |NoGrad.end()|

------------------------------------------------------------------------------
Tensor.backward(create_graph=0, retain_outgrad=0)         *Tensor.backward()*
Backpropagate from the current tensor to obtain the gradient.
This function accumulates the gradient in the .grad attribute of the leaves.
Therefore, multiple calling without |Tensor.cleargrad()| method will add a new
gradient into the accumulated gradient to perform double-backprop.

    Arguments:
        create_graph (bool):
            Construct a computational graph for backpropergation to compute
            higher-order derivatives.

        retain_outgrad (bool):
            Holds the gradient of the variable output by each function.
            if y=f(x), y.grad holds the gradient of df/dy.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#mul(x, 3.0)
        call y.backward()
<

------------------------------------------------------------------------------
Tensor.cleargrad()                                      *Tensor.cleargrad()*
Resets the retained gradient.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 3.0)
        call y.backward(1)
        let gx1 = x.grad

        call x.cleargrad()
        call gx1.backward()
        let gx2 = x.grad
<

------------------------------------------------------------------------------
Tensor.a(other)                                                 *Tensor.a()*
This method is the same as autograd#add(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.a(3.0)
<

------------------------------------------------------------------------------
Tensor.m(other)                                                 *Tensor.m()*
This method is the same as autograd#mul(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.m(3.0)
<

------------------------------------------------------------------------------
Tensor.s(other)                                                 *Tensor.s()*
This method is the same as autograd#sub(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.s(3.0)
<

------------------------------------------------------------------------------
Tensor.d(other)                                                 *Tensor.d()*
This method is the same as autograd#div(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
             operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.d(3.0)
<

------------------------------------------------------------------------------
Tensor.p(other)                                                 *Tenspr.p()*
This method is the same as autograd#pow(self, other).

    Arguments:
        other (Tensor, `List`, `Float`, `Number`):
            operand of the right-hand side.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.p(3.0)
<

------------------------------------------------------------------------------
Tensor.n()                                                      *Tensor.n()*
This method is the same as autograd#mul(self, -1.0).

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.n()
<

------------------------------------------------------------------------------
Tensor.T()                                                      *Tensor.T()*
This method is the same as autograd#transpose(self).

    Example: >
        let x0 = autograd#randn(2, 3)
        let x1 = autograd#randn(2, 3)
        let y = autograd#matmul(x0, x1.T())
<

------------------------------------------------------------------------------
Tensor.reshape(s1, s2, ..., sn), Tensor.reshape(shape)    *Tensor.reshape()*
This method is the same as autograd#reshape(self, [s1, s2, ..., sn]) or
autograd#reshape(self, shape).

    Arguments
        shape (`List` or variadic):
            When only one argument is given, if its type is v:t_list, it is
            considered a shape; otherwise, a:000 is considered a shape.

    Example: >
        let x0 = autograd#randn(2, 3)
        let x1 = x0.reshape(3, 2)
        let x2 = x0.reshape([3, 2])
<

------------------------------------------------------------------------------
Tensor.clone()                                              *Tensor.clone()*
Create a new tensor by copying the .data and .shape of the tensor. The created
tensor is completely independent of the original tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = x.clone()
<

------------------------------------------------------------------------------
Tensor.detach()                                            *Tensor.detach()*
It returns a new tensor detached from the current graph. However, returned
tensor shares the same data and shape attribute.

    Example: >
        let x = autograd#randn(2, 3).m(0.01).detach()
<

------------------------------------------------------------------------------
Function.apply(x1, x2, ..., xn)                           *Function.apply()*
It constructs computed graphs after calling the forward method of a
differentiable function of your own definition. If the |Function.forward()|
method returns a `List` with one element, it returns a Tensor object. If
|Function.forward()| method returns a `List` with two or more elements, it
returns a `List`.
For details, see |autograd#Function()|

    Arguments:
        x1, x2, ..., xn (variadic of Tensor, `List`, `Float`, or `Number`):
            All arguments are converted to Tensor.

------------------------------------------------------------------------------
Function.forward(xs)                                    *Function.forward()*
Forward propagation of the function.
When defining your own, you must return a `List` of Tensors in the return
value. For details, see |autograd#Function()|

    Arguments:
        xs (`List` of Tensor):
            A `List` of Tensors is passed as an argument.

------------------------------------------------------------------------------
Function.backward(gys)                                 *Function.backward()*
Backward propagation of the function.
A `List` with Tensors whose elements refer to the .grad attribute of the
Tensor output by |Function.forward()| is passed as an argument. The function
used in backward must be a differentiable Function object, and the return
value must be a `List` of Tensors.
For details, see |autograd#Function()|

    Arguments:
        gys (`List` of Tensor):
            A `List` of Tensors is passed as an argument.

------------------------------------------------------------------------------
NoGrad.end()                                                  *NoGrad.end()*
Exit from the mode without backpropagation.
NoGrad objects can be created with |autograd#no_grad()|.

    Example: >
        let x = autograd#tensor(2.0)
        let handle = autograd#no_grad()
        let y = autograd#pow(x, 4.0)
        call handle.end()
<

==============================================================================
FUNCTIONS                                           *vim-autograd-functions*
    Tensor Constructor
        autograd#tensor()                                |autograd#tensor()|
        autograd#as_tensor()                          |autograd#as_tensor()|
        autograd#zeros()                                  |autograd#zeros()|
        autograd#zeros_like()                        |autograd#zeros_like()|
        autograd#ones()                                    |autograd#ones()|
        autograd#ones_like()                          |autograd#ones_like()|
        autograd#rand()                                    |autograd#rand()|
        autograd#uniform()                              |autograd#uniform()|
        autograd#randn()                                  |autograd#randn()|
        autograd#normal()                                |autograd#normal()|

    Function Constructor
        autograd#Function()                            |autograd#Function()|

    Differentiable Functions
        autograd#add()                                      |autograd#add()|
        autograd#mul()                                      |autograd#mul()|
        autograd#sub()                                      |autograd#sub()|
        autograd#div()                                      |autograd#div()|
        autograd#pow()                                      |autograd#pow()|
        autograd#sqrt()                                    |autograd#sqrt()|
        autograd#log()                                      |autograd#log()|
        autograd#exp()                                      |autograd#exp()|
        autograd#sin()                                      |autograd#sin()|
        autograd#cos()                                      |autograd#cos()|
        autograd#tanh()                                    |autograd#tanh()|
        autograd#abs()                                      |autograd#abs()|
        autograd#sign()                                    |autograd#sign()|
        autograd#sum()                                      |autograd#sum()|
        autograd#broadcast_to()                    |autograd#broadcast_to()|
        autograd#sum_to()                                |autograd#sum_to()|
        autograd#max()                                      |autograd#max()|
        autograd#maximum()                              |autograd#maximum()|
        autograd#transpose()                          |autograd#transpose()|
        autograd#matmul()                                |autograd#matmul()|
        autograd#reshape()                              |autograd#reshape()|
        autograd#flatten()                              |autograd#flatten()|

    Mathematical Utilities
        autograd#manual_seed()                      |autograd#manual_seed()|
        autograd#random()                                |autograd#random()|
        autograd#fmax()                                    |autograd#fmax()|
        autograd#pi()                                        |autograd#pi()|
        autograd#grad()                                    |autograd#grad()|
        autograd#no_grad()                              |autograd#no_grad()|
        autograd#elementwise()                      |autograd#elementwise()|

    Data
        autograd#data#shuffle()                    |autograd#data#shuffle()|

    Image
        autograd#image#open_b()                    |autograd#image#open_b()|
        autograd#image#save_b()                    |autograd#image#save_b()|

    Utilities
        autograd#utils#dump_graph()            |autograd#utils#dump_graph()|
        autograd#utils#numerical_grad()    |autograd#utils#numerical_grad()|
        autograd#utils#gradcheck()              |autograd#utils#gradcheck()|

------------------------------------------------------------------------------
autograd#tensor(data)                                    *autograd#tensor()*
It creates a Tensor object based on the arguments.

    Arguments:
        data (`List`, `Float`, `Number`):
            `Float` and `Number` are generated as Tensor of shape in [1]. It
            accepts a `List` of multidimensional arrays as arguments and
            generates a Tensor of that shape. Arrays such as [[1], [2, 3]]
            will result in an error. By the way, `Number` is converted to
            `Float`.

    Example: >
        let x = autograd#tensor([[1, 2, 3], [4, 5, 6]])
        call assert_equal([2, 3], x.shape)
        call assert_equal([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], x.data)
<


------------------------------------------------------------------------------
autograd#as_tensor(data)                              *autograd#as_tensor()*
Convert data to Tensor.

    Arguments:
        data (Tensor, `List`, `Float`, `Number`):
            Tensors are returned as is, and others are converted to Tensors
            and returned.

    Example: >
        let x = autograd#as_tensor([2, 3, 4])
<

------------------------------------------------------------------------------
autograd#zeros(shape)                                     *autograd#zeros()*
Create a Tensor filled with zero with the shape given by the argument.

    Arguments:
        shape (`List`):
            `List` with `Number` as element.

    Example: >
        let x = autograd#zeros([2, 3])
<

------------------------------------------------------------------------------
autograd#zeros_like(tensor)                          *autograd#zeros_like()*
Create a Tensor filled with zero in the same shape as the argument.

    Arguments:
        tensor (Tensor):
            The shape will be the same as the output tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#zeros_like(x)
<

------------------------------------------------------------------------------
autograd#ones(shape)                                       *autograd#ones()*
Create a Tensor filled with one with the shape given by the argument.

    Arguments:
        shape (`List`):
            `List` with `Number` as element.

    Example: >
        let x = autograd#ones([2, 3])
<

------------------------------------------------------------------------------
autograd#ones_like(tensor)                            *autograd#ones_like()*
Create a Tensor filled with one in the same shape as the argument.

    Arguments:
        tensor (Tensor):
            The shape will be the same as the output tensor.

    Example: >
        let x = autograd#randn(2, 3)
        let y = autograd#ones_like(x)
<

------------------------------------------------------------------------------
autograd#rand(s1, s2, ..., sn)                             *autograd#rand()*
Create a Tensor of the same shape as the argument with the random value
sampled from a uniform distribution as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#rand(2, 3)
<

------------------------------------------------------------------------------
autograd#uniform(low=0.0, high=1.0, shape=[1])          *autograd#uniform()*
    Example: >
<

------------------------------------------------------------------------------
autograd#randn(s1, s2, ..., sn)                           *autograd#randn()*
Create a Tensor of the same shape as the argument with the random value
sampled from a standard uniform distribution sampled from a normal
distribution with mean 0.0 and standard deviation 1.0 as elements.

    Arguments:
        s1, s2, ..., sn (variadic of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#randn(2, 3)
<

------------------------------------------------------------------------------
autograd#normal(mean=0.0, std=1.0, shape=[1])            *autograd#normal()*
Generate a normal distribution with arbitrary mean and standard deviation.

    Arguments:
        mean (`Float`):
            Average value of `Float` type.

        std (`Float`):
            Standard deviation of `Float` type.

        shape (`List` of `Number`):
            The output tensor has the same as this shape.

    Example: >
        let x = autograd#normal(0.0, 2.0, [2, 3])
<

------------------------------------------------------------------------------
autograd#Function(name)                                *autograd#Function()*
It defines a differentiable function.
At this time, you must also define functions with a dict suffix of
{name}_forward() and {name}_backward() to the name specified in the name
argument. Defining your own Function object is more efficient than using the
|autograd#add()| and |autograd#mul()| provided as autograd API, since it does
not generate intermediate objects in the computed graph. The
|Function.forward()| and |Function.backward()| methods refer to the
{name}_forward and {name}_backward methods, respectively.

    Arguments:
        name (string):
            This argument must also have a scope prefix such as `s:`.

    Example: >
        function! nn#relu(x) abort
          return autograd#Function('nn#relu').apply(a:x)
        endfunction

        function! nn#relu_forward(xs) dict abort
          return [autograd#elementwise(a:xs, {a -> a > 0.0 ? a : 0.0})]
        endfunction

        function! nn#relu_backward(gys) dict abort
          let l:x = self.inputs[0]
          let l:mask = autograd#elementwise([l:x], {a -> a > 0.0})
          return [autograd#mul(a:gys[0], l:mask)]
        endfunction
<

------------------------------------------------------------------------------
autograd#add(input, other)                                  *autograd#add()*
Add like input + other.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):

        other (Tensor, `List`, `Float`, `Number`):

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#add(x, 4.0)
<

------------------------------------------------------------------------------
autograd#mul(input, other)                                  *autograd#mul()*
Multiply like input * other.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(4.0)
        let y = autograd#mul(x, 2.0)
<

------------------------------------------------------------------------------
autograd#sub(input, other)                                  *autograd#sub()*
Subtract like input - other.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(5.0)
        let y = autograd#sub(x, 2.0)
<

------------------------------------------------------------------------------
autograd#div(input, other)                                  *autograd#div()*
Divide like input / other.

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(6.0)
        let y = autograd#div(x, 2.0)
<

------------------------------------------------------------------------------
autograd#pow(input, other)                                  *autograd#pow()*
Power like input ** other (input ^ other).

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of other.

        other (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor. It is also
            broadcast if it is smaller than the shape of input.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#pow(x, 5.0)
<

------------------------------------------------------------------------------
autograd#sqrt(input)                                       *autograd#sqrt()*
Calculate square root of input. This is the same as autograd#pow(input, 0.5).

    Arguments:
        input (Tensor, `List`, `Float`, `Number`):
            This argument is automatically converted to a Tensor.

    Example: >
        let x = autograd#tensor(2.0)
        let y = autograd#sqrt(x)
<

------------------------------------------------------------------------------
autograd#log(input)                                         *autograd#log()*
    Example: >
<

------------------------------------------------------------------------------
autograd#exp(input)                                         *autograd#exp()*
    Example: >
<

------------------------------------------------------------------------------
autograd#sin(theta)                                         *autograd#sin()*
    Example: >
<

------------------------------------------------------------------------------
autograd#cos(theta)                                         *autograd#cos()*
    Example: >
<

------------------------------------------------------------------------------
autograd#tanh(theta)                                       *autograd#tanh()*
    Example: >
<

------------------------------------------------------------------------------
autograd#abs(input)                                         *autograd#abs()*
    Example: >
<

------------------------------------------------------------------------------
autograd#sign(input)                                       *autograd#sign()*
    Example: >
<

------------------------------------------------------------------------------
autograd#sum(input)                                         *autograd#sum()*
    Example: >
<

------------------------------------------------------------------------------
autograd#broadcast_to(input, shape)                *autograd#broadcast_to()*
    Example: >
<

------------------------------------------------------------------------------
autograd#sum_to(input, shape)                            *autograd#sum_to()*
    Example: >
<

------------------------------------------------------------------------------
autograd#max(input)                                         *autograd#max()*
    Example: >
<

------------------------------------------------------------------------------
autograd#maximum(input, other)                          *autograd#maximum()*
    Example: >
<

------------------------------------------------------------------------------
autograd#transpose(input)                             *autograd#transpose()*
    Example: >
<

------------------------------------------------------------------------------
autograd#matmul(input, other)                            *autograd#matmul()*
    Example: >
<

------------------------------------------------------------------------------
autograd#reshape(input, shape)                          *autograd#reshape()*
    Example: >
<

------------------------------------------------------------------------------
autograd#flatten(input)                                 *autograd#flatten()*
    Example: >
<

------------------------------------------------------------------------------
autograd#manual_seed()                              *autograd#manual_seed()*
It sets the seed of the random functions provided by vim-autograd. By default,
the seed is a random value returned by `srand()`.

    Example: >
        call autograd#manual_seed(42)
<

------------------------------------------------------------------------------
autograd#random()                                        *autograd#random()*
It returns a random scalar value between 0.0 and 1.0.

    Example: >
        let x = autograd#random()
<

------------------------------------------------------------------------------
autograd#fmax(list_obj)                                    *autograd#fmax()*
It returns the maximum value of a `List` whose elements are `Float` values.

    Arguments:
         list_obj (`List`):
             A list of `Float' or `Number`.

    Example: >
        let x = autograd#fmax([2, 3, 4, 5, 6])
<

------------------------------------------------------------------------------
autograd#pi()                                                *autograd#pi()*
It returns a scalar value that approximates pi.

    Example: >
        let x = autograd#pi()
<

------------------------------------------------------------------------------
autograd#grad()                                            *autograd#grad()*
    Example: >
<

------------------------------------------------------------------------------
autograd#no_grad()                                      *autograd#no_grad()*
Switch to a mode that does not create computed graphs to save computation cost
and memory consumption. Backpropagation is not available for functions
computed in this mode. To exit this mode, call the |NoGrad.end()| method of
the returned handle.

    Example: >
        let x = autograd#tensor(2.0)
        let handle = autograd#no_grad()
        let y = autograd#pow(x, 4.0)
        call handle.end()
<

------------------------------------------------------------------------------
autograd#elementwise()                              *autograd#elementwise()*
    Example: >
<

------------------------------------------------------------------------------
autograd#data#shuffle()                            *autograd#data#shuffle()*
    Example: >
<

------------------------------------------------------------------------------
autograd#image#open_b()                            *autograd#image#open_b()*
    Example: >
<

------------------------------------------------------------------------------
autograd#image#save_b()                            *autograd#image#save_b()*
    Example: >
<

------------------------------------------------------------------------------
autograd#utils#dump_graph()                    *autograd#utils#dump_graph()*
    Example: >
<

------------------------------------------------------------------------------
autograd#utils#numerical_grad()            *autograd#utils#numerical_grad()*
    Example: >
<

------------------------------------------------------------------------------
autograd#utils#gradcheck()                      *autograd#utils#gradcheck()*
    Example: >
<

==============================================================================
LICENSE                                               *vim-autograd-license*

MIT License Copyright (c) 2022 pit-ray

The full text is available at
https://github.com/pit-ray/vim-autograd/blob/main/LICENSE.


==============================================================================
MAINTAINERS                                       *vim-autograd-maintainers*

* pit-ray (author, maintainer): https://github.com/pit-ray


vim:tw=78:ts=8:ft=help-norl:
